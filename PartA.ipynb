{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "PartA.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g-uY2OfBwabt",
        "outputId": "d7fd9010-7728-4c70-9948-4a81c461a2c6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[K     |████████████████████████████████| 1.8 MB 5.4 MB/s \n",
            "\u001b[K     |████████████████████████████████| 181 kB 49.2 MB/s \n",
            "\u001b[K     |████████████████████████████████| 144 kB 53.5 MB/s \n",
            "\u001b[K     |████████████████████████████████| 63 kB 1.3 MB/s \n",
            "\u001b[?25h  Building wheel for pathtools (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ],
      "source": [
        "#install wandb\n",
        "!pip install -q wandb --upgrade\n",
        "import wandb"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Importin all the used libraries \n",
        "import sys\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "import pandas as pd\n",
        "from tensorflow.keras import layers, Model,Input\n",
        "from tensorflow.keras.layers.experimental.preprocessing import Rescaling\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.preprocessing import image_dataset_from_directory as image_generator\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator"
      ],
      "metadata": {
        "id": "nr9b5SEtwguz"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Path to the directory\n",
        "Data_Directory = '/content/drive/MyDrive/inaturalist_12K/train'"
      ],
      "metadata": {
        "id": "-PeXcBThwjBC"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Generating data from the directory \n",
        "def Data_Generator(Data_Aug):\n",
        "  if Data_Aug ==False:\n",
        "    image_Data_generator=ImageDataGenerator(rescale=1./255,validation_split=0.1,preprocessing_function=None,data_format=None)\n",
        "  else:\n",
        "    image_Data_generator=ImageDataGenerator(rescale=1./255,validation_split=0.1,featurewise_center=False,\n",
        "    zca_epsilon=1e-06,\n",
        "    rotation_range=25,\n",
        "    width_shift_range=0.3,\n",
        "    height_shift_range=0.3,\n",
        "    shear_range=0.0,\n",
        "    zoom_range=0.0,\n",
        "    channel_shift_range=0.0,\n",
        "    fill_mode='nearest',\n",
        "    cval=0.0,\n",
        "    preprocessing_function=None,\n",
        "    data_format='channels_last',\n",
        "    dtype=None)\n",
        "  \n",
        "  #Final Train and Validation data\n",
        "  Train_Data=image_Data_generator.flow_from_directory(Data_Directory, shuffle = True,seed = 20, subset = 'training')\n",
        "  Validation_Data=image_Data_generator.flow_from_directory(Data_Directory, shuffle = True,seed = 20, subset = 'validation')\n",
        "\n",
        "  #You Can select only some percent of data\n",
        "  #Train_Data = Train_Data.take(int(0.20* len(Train_Data)))\n",
        "  #Validation_Data = Validation_Data.take(int(0.20*len(Validation_Data)))\n",
        "\n",
        "  return Train_Data,Validation_Data\n"
      ],
      "metadata": {
        "id": "_cgoK9bZwl7E"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Setting the Sweep Parameters\n",
        "Sconfig = {'name': 'Test-Sweep', 'method': 'grid'}\n",
        "parameters_dict = {\n",
        "                   'FirstLayerFilters': {'values': [64, 32]},\n",
        "                   'DenseSize': {'values': [128, 64, 32]},\n",
        "                   'FilterOrg': {'values': [1,0.5,  2]},\n",
        "                   'BatchNorm': {'values': [True, False]},\n",
        "                   'DataAug': {'values': [True, False]},\n",
        "                   'Dropout': {'values': [0.2, 0.0, 0.3]},\n",
        "                   'KerSize': {'values': [3, 5, 7]}}\n",
        "#main sweep config\n",
        "Sconfig['parameters'] = parameters_dict"
      ],
      "metadata": {
        "id": "t5HStm00wolN"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Building the model\n",
        "def Main_Model(Train_Data,Validation_Data,Shape,Activation,PoolSize,First_Layer_Filter,Filter_Org,Convolution_Layers,optimizer,Dense_Size,Kernel_Size,Data_Aug,Dropout,Batch_Normalization,epoch):\n",
        "  model=tf.keras.Sequential()\n",
        "  model.add(layers.Input(Shape))\n",
        "  Filter_Size=[]\n",
        "\n",
        "  for Layer_Val in range(Convolution_Layers):\n",
        "    filter_size=(Filter_Org**Layer_Val)*First_Layer_Filter\n",
        "    model.add(layers.Conv2D(kernel_size=(Kernel_Size,Kernel_Size),filters=filter_size))\n",
        "    \n",
        "    if Batch_Normalization:\n",
        "      model.add(layers.BatchNormalization(axis=-1))\n",
        "  \n",
        "  model.add(layers.Activation(Activation))\n",
        "  model.add(layers.MaxPooling2D(pool_size=(PoolSize,PoolSize),strides=None))\n",
        "\n",
        "  if Dropout>0:\n",
        "    model.add(layers.Dropout(Dropout))\n",
        "  \n",
        "\n",
        "  model.add(layers.Flatten())\n",
        "  model.add(layers.Dense(Dense_Size))\n",
        "\n",
        "\n",
        "  if Batch_Normalization:\n",
        "    model.add(layers.BatchNormalization(axis=-1))\n",
        "  model.add(layers.Activation(Activation))\n",
        "\n",
        "  if Dropout>0:\n",
        "    model.add(layers.Dropout(Dropout))\n",
        "  model.add(layers.Dense(10,activation='softmax'))\n",
        "  return model"
      ],
      "metadata": {
        "id": "R5EHlvOzwrs0"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def Start_Sweep(config=Sconfig):\n",
        "  with wandb.init(config=config):\n",
        "    config=wandb.config\n",
        "    wandb.run.name=f\"Dropout_{config.Dropout}_FilterOrg_{config.FilterOrg}_DenseSize_{config.DenseSize}_BatchNorm_{config.BatchNorm}_FirstLayerFilters_{config.FirstLayerFilters}_KerSize_{config.KerSize}_DataAug_{config.DataAug}\"\n",
        "    \n",
        "    #Initializing the Parameters through sweep config\n",
        "    Shape=(256,256,3)\n",
        "    Activation='relu'\n",
        "    PoolSize=2\n",
        "    First_Layer_Filter=config.FirstLayerFilters\n",
        "    Filter_Org=config.FilterOrg\n",
        "    Convolution_Layers=5\n",
        "    optimizer='adam'\n",
        "    Dense_Size=config.DenseSize\n",
        "    Kernel_Size=config.KerSize\n",
        "    Data_Aug=config.DataAug\n",
        "    Dropout=config.Dropout\n",
        "    Batch_Normalization=config.BatchNorm\n",
        "    epoch=50\n",
        "    Train_Data,Validation_Data=Data_Generator(Data_Aug)\n",
        "\n",
        "    #Calling Main_Model\n",
        "    model=Main_Model(Train_Data,Validation_Data,Shape,Activation,PoolSize,First_Layer_Filter,Filter_Org,Convolution_Layers,optimizer,Dense_Size,Kernel_Size,Data_Aug,Dropout,Batch_Normalization,epoch)\n",
        "    model.compile(optimizer=optimizer,loss='categorical_crossentropy',metrics = [\"accuracy\"])\n",
        "    model.fit(Train_Data,epochs=epoch,validation_data=Validation_Data,callbacks=[wandb.keras.WandbCallback(),tf.keras.callbacks.EarlyStopping(monitor='val_accuracy', patience=4)])\n",
        "  "
      ],
      "metadata": {
        "id": "1xHzDNrWw3wy"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Making Sweep Functionality\n",
        "sweep_id = wandb.sweep(Sconfig, project = 'DL_Assignment2_PartA(1)')\n",
        "wandb.agent(sweep_id, function=Start_Sweep)"
      ],
      "metadata": {
        "id": "fUdhZ5sTLXaz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Code Without Wandb\n",
        "#Uncomment to Execute\n",
        "\"\"\"#Pass the Input Parmeters\n",
        "Parameters = sys.argv[1:]\n",
        "Shape=(256,256,3)\n",
        "Activation=Parameters[1]\n",
        "PoolSize=int(Parameters[3])\n",
        "First_Layer_Filter=int(Parameters[5])\n",
        "Filter_Org=float(Parameters[7])\n",
        "Convolution_Layers=int(Parameters[9])\n",
        "optimizer=Parameters[11]\n",
        "Dense_Size=int(Parameters[13])\n",
        "Kernel_Size=int(Parameters[15])\n",
        "Data_Aug=Parameters[17]\n",
        "Dropout=float(Parameters[19])\n",
        "Batch_Normalization=Parameters[21]\n",
        "epoch=int(Parameters[23])\n",
        "\n",
        "#Generating the Data\n",
        "Train_Data,Validation_Data=Data_Generator(True)\n",
        "#Building the Model\n",
        "model=Main_Model(Train_Data,Validation_Data,Shape,Activation,PoolSize,First_Layer_Filter,Filter_Org,Convolution_Layers,optimizer,Dense_Size,Kernel_Size,Data_Aug,Dropout,Batch_Normalization,epoch)\n",
        "model.compile(optimizer='adam',loss='categorical_crossentropy',metrics = [\"accuracy\"])\n",
        "model.fit(Train_Data,epochs=10,validation_data=Validation_Data)\n",
        "\n",
        "\n",
        "\"\"\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 124
        },
        "id": "N6pCzzNFw-kI",
        "outputId": "f5882716-44c4-4483-c4fb-b3c55240d0cb"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'#Pass the Input Parmeters\\nParameters = sys.argv[1:]\\nShape=(256,256,3)\\nActivation=Parameters[1]\\nPoolSize=int(Parameters[3])\\nFirst_Layer_Filter=int(Parameters[5])\\nFilter_Org=float(Parameters[7])\\nConvolution_Layers=int(Parameters[9])\\noptimizer=Parameters[11]\\nDense_Size=int(Parameters[13])\\nKernel_Size=int(Parameters[15])\\nData_Aug=Parameters[17]\\nDropout=float(Parameters[19])\\nBatch_Normalization=Parameters[21]\\nepoch=int(Parameters[23])\\n\\n#Generating the Data\\nTrain_Data,Validation_Data=Data_Generator(True)\\n#Building the Model\\nmodel=Main_Model(Train_Data,Validation_Data,Shape,Activation,PoolSize,First_Layer_Filter,Filter_Org,Convolution_Layers,optimizer,Dense_Size,Kernel_Size,Data_Aug,Dropout,Batch_Normalization,epoch)\\nmodel.compile(optimizer=\\'adam\\',loss=\\'categorical_crossentropy\\',metrics = [\"accuracy\"])\\nmodel.fit(Train_Data,epochs=10,validation_data=Validation_Data)\\n\\n\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    }
  ]
}